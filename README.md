C√≥digo experimental de RAG - Retreavel-Augment Generation, para rodar localmente, utilizando:

    - Streamlit
    - LangChain
    - Hugging Face
    - Ollama com DeepSeek


Funcionalidade do C√≥digo:

    Interface Streamlit:
        Cria um t√≠tulo para a aplica√ß√£o web: "üìÑ RAG System with DeepSeek R1 & Ollama".
        Fornece um widget para o usu√°rio fazer o upload de um arquivo PDF.

    Carregamento do PDF:
        Quando um arquivo PDF √© carregado, ele √© salvo temporariamente como temp.pdf.
        Utiliza PDFPlumberLoader do Langchain para carregar o conte√∫do do PDF em objetos Document.

    Chunking Sem√¢ntico:
        Emprega SemanticChunker do Langchain Experimental, utilizando embeddings do Hugging Face
        (HuggingFaceEmbeddings), para dividir o texto do PDF em chunks semanticamente relevantes. A ideia por tr√°s
        do chunking sem√¢ntico √© agrupar frases e par√°grafos que compartilham um significado similar, o que pode
        melhorar a relev√¢ncia do contexto recuperado.

    Cria√ß√£o do Vetor Store:
        Utiliza HuggingFaceEmbeddings para gerar embeddings para cada chunk de texto.
        Cria um √≠ndice FAISS (uma biblioteca para busca de similaridade eficiente) a partir dos embeddings e dos
        documentos (chunks). Este √≠ndice permitir√° a busca r√°pida de chunks relevantes para uma dada pergunta.

    Configura√ß√£o do Retriever:
        Cria um retriever (vector.as_retriever) a partir do √≠ndice FAISS.
        Configura a busca para ser por similaridade (search_type="similarity") e para retornar os 3 chunks mais
        relevantes (search_kwargs={"k": 3}).

    Configura√ß√£o do LLM:
        Inicializa um modelo de linguagem grande (llm) utilizando Ollama, especificamente o modelo deepseek-r1:1.5b.
        Ollama permite executar modelos LLM localmente.

    Cria√ß√£o do Prompt:
        Define um template de prompt (PromptTemplate) que ser√° usado para instruir o LLM a responder √† pergunta do
        usu√°rio com base no contexto recuperado. O prompt inclui as vari√°veis {context} (onde os chunks relevantes
        ser√£o inseridos) e {question} (a pergunta do usu√°rio).

    Cria√ß√£o das Chains:
        LLMChain: Cria uma chain que combina o LLM (Ollama) com o prompt.
        StuffDocumentsChain: Cria uma chain para combinar os m√∫ltiplos documentos (chunks) recuperados em uma √∫nica
        string de contexto que ser√° inserida no prompt do LLM. A estrat√©gia "stuff" simplesmente junta todos os
        documentos em um √∫nico contexto.
        RetrievalQA: Cria a chain principal de RAG, que orquestra a recupera√ß√£o dos documentos relevantes
        (via retriever) e a gera√ß√£o da resposta pelo LLM (via combine_documents_chain).

    Interface de Pergunta e Resposta:
        Fornece um campo de texto (st.text_input) para o usu√°rio inserir sua pergunta sobre o documento.
        Quando o usu√°rio faz uma pergunta:
            A pergunta √© passada para a chain qa.
            A resposta gerada pelo LLM √© extra√≠da do resultado (["result"]).
            A resposta √© exibida na interface Streamlit.
