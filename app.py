"""
Exemplo de RAG utilizando Streamlit, DeepSeek local com Ollama.

O Streamlit faz uma UI

O programa faz o processo resumido:

1. Carga com PDFlumberLoader
2. Divide em Chunks com SemanticChunker(HuggingFaceEmbeddings()) e text_splitter.split_documents(docs)
3. Cria embeddings com  HuggingFaceEmbeddings()
4. Vetoriza os Stores, com FAISS
5.
"""

import streamlit as st
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA

# 1. Streamlit UI
st.title("üìÑ RAG System with DeepSeek R1 & Ollama")

"""
2. Carrega o PDF
"""

uploaded_file = st.file_uploader("Upload your PDF file here", type="pdf")

if uploaded_file:
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getvalue())

    loader = PDFPlumberLoader("temp.pdf")
    docs = loader.load()

    """
    3. Chunking Sem√¢ntico
    SemanticChunker do Langchain Experimental, utilizando embeddings do Hugging Face (HuggingFaceEmbeddings), 
    para dividir o texto do PDF em chunks semanticamente relevantes. A ideia por tr√°s do chunking sem√¢ntico √© agrupar 
    frases e par√°grafos que compartilham um significado similar, o que pode melhorar a relev√¢ncia 
    do contexto recuperado."""

    text_splitter = SemanticChunker(HuggingFaceEmbeddings())
    documents = text_splitter.split_documents(docs)

    """
    4. Cria√ß√£o do Vetor Store
    Utiliza HuggingFaceEmbeddings para gerar embeddings para cada chunk de texto.
    Cria um √≠ndice FAISS (uma biblioteca para busca de similaridade eficiente) a partir dos embeddings e dos 
    documentos (chunks). Este √≠ndice permitir√° a busca r√°pida de chunks relevantes para uma dada pergunta."""

    embedder = HuggingFaceEmbeddings()
    vector = FAISS.from_documents(documents, embedder)

    """
    5. Cria√ß√£o do Retriever
    Cria um retriever (vector.as_retriever) a partir do √≠ndice FAISS.
    Configura a busca para ser por similaridade (search_type="similarity") e para retornar os 3 chunks mais 
    relevantes (search_kwargs={"k": 3})."""

    retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    """
    6. Configura√ß√£o do LLM
    Inicializa um modelo de linguagem grande (llm) utilizando Ollama, especificamente o 
    modelo deepseek-r1:1.5b. Ollama permite executar modelos LLM localmente."""

    llm = Ollama(model="deepseek-r1:1.5b")

    """
    7. Cria√ß√£o do Prompt
    Define um template de prompt (PromptTemplate) que ser√° usado para instruir o LLM a responder √† 
    pergunta do usu√°rio com base no contexto recuperado. O prompt inclui as vari√°veis {context} (onde os chunks
    relevantes ser√£o inseridos) e {question} (a pergunta do usu√°rio)."""

    prompt = """
    Use the following context to answer the question.
    Context: {context}
    Question: {question}
    Answer:"""

    QA_PROMPT = PromptTemplate.from_template(prompt)

    """
    8. Cria√ß√£o das Chains
    LLMChain: Cria uma chain que combina o LLM (Ollama) com o prompt.
    StuffDocumentsChain: Cria uma chain para combinar os m√∫ltiplos documentos (chunks) recuperados em uma √∫nica 
    string de contexto que ser√° inserida no prompt do LLM. A estrat√©gia "stuff" simplesmente junta todos os documentos 
    em um √∫nico contexto.
    RetrievalQA: Cria a chain principal de RAG, que orquestra a recupera√ß√£o dos documentos relevantes (via retriever) 
    e a gera√ß√£o da resposta pelo LLM (via combine_documents_chain).
     """

    llm_chain = LLMChain(llm=llm, prompt=QA_PROMPT)
    combine_documents_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="context")

    qa = RetrievalQA(combine_documents_chain=combine_documents_chain, retriever=retriever)

    """
    9. Interface de Pergunta e Resposta
    Fornece um campo de texto (st.text_input) para o usu√°rio inserir sua pergunta sobre o documento.
    Quando o usu√°rio faz uma pergunta:

    A pergunta √© passada para a chain qa.
    A resposta gerada pelo LLM √© extra√≠da do resultado (["result"]).
    A resposta √© exibida na interface Streamlit."""

    user_input = st.text_input("Ask a question about your document:")

    if user_input:
        response = qa(user_input)["result"]
        st.write("**Response:**")
        st.write(response)
